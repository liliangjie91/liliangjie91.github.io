---
title: 古早博文-机器学习-损失函数
date: 2017-12-31 19:51:55
categories: 
    - 技术
    - 机器学习
tags:
    - 机器学习
mathjax: true
---
#### 0，综述
损失函数用于评价模型好坏。一个统计学习方法**基本上**由三个部分组成：
模型+策略+算法
**1，模型**，$y=f(x)$，即输入样本特征，可以返回样本值或概率值的函数
**2，策略**，有了模型，如何确定模型中的参数呢？如何根据训练数据拟合一个不错的模型呢？这就是一个训练策略的问题。基本上就是：减小模型误差(损失)或增大模型收益(如最大似然)，这两种方式是可以互相转化的。通常我们会采取减小模型误差的方式。那么，就需要选取一个函数来评价模型的损失(误差)即损失函数。不同的损失函数适用不同的任务。
**3，算法**，有了损失函数还不够，我们的目的是利用数据**降低**损失函数。这里就会有一些优化算法适用于降低损害函数。这是一个优化问题。如果损失函数简单，可以直接计算解析解，那很容易就能求得最优参数从而确定模型。但往往在多维数据下，难以甚至无法计算解析解，此时，就需要一些优化算法来逐步逼近最优解。例如梯度下降法，牛顿法，以及一些优化转换方法例如拉格朗日对偶性。


#### 1，0-1损失函数
$$L=\begin{cases} 0, \text{ if }y=f(x) \\ 1, \text{ if }y\ne f(x) \end{cases}$$
此损失函数形式简单，但是
1，不可导不易优化。
2，一刀切，没有体现f(x)接近和远离y时损失的差异
所以使用有局限性。适用于计数类型的模型，例如k近邻。

#### 2，合页损失函数
一般模型为
$y=sign(wx+b)$
合页损失，重要的组成部分称为函数间隔$y_i(wx_i+b)$，分对则大于0，分错小于0
##### 2.1 感知机损失函数
$$L=\sum[-y_i(wx_i+b)]_{+}$$

##### 2.2 SVM损失函数
$$L=\sum[-y_i(wx_i+b)+1]_{+}+\lambda|w|^2$$

##### 2.3 讨论
方括号+的意思是，括号内的值如果小于0，则值等于0，括号内值大于0则保持原值。
两者经验损失部分十分相似，只不过相差1，表示SVM的经验损失比感知机更严格(即使分对了，但$y_i(wx_i+b)<1$,也会带来损失)
我认为两者的损失函数(经验损失)都是合页损失函数(以函数间隔为x轴)，只不过SVM的右偏了一格。而SVM真正比感知机优越的地方在于其L2正则，当然，可以引入核函数则是其更重要的性质与优点了。
另外，SVM的这个损失函数等价于他的最优化问题。把带约束的最优化问题转换为不带约束的最优化问题，在数学上的转换与证明也是十分巧妙的。

#### 3，以熵或基尼指数度量的损失函数
决策树的损失函数
决策树的损失函数通常用于树的剪枝。
$$C_{\alpha}(T)=C(T)+\alpha|T|$$
$$C(T)=\sum_{t=1}^{|T|}N_tH_t(T)$$
$H_t(T)$表示t叶节点上的熵或基尼指数。
通过计算剪枝前后的损失函数大小来判断是否剪枝。

#### 4，交叉熵损失函数(Cross Entropy Loss)
##### 4.1 K-L散度(Kullback–Leibler divergence)
又称相对熵(Relative Entropy)
举个例子：

>假设我们获取了某一地区人口年龄的样本数据D
>$$D=[(0,18),(1,16),...,(30,22),...(89,1),...(98,0.5)]$$
>代表了从0岁到98岁的人口数量，单位万人。
此时，我们想用一个分布来拟合这批数据，以预测本地区总体年龄分布。
A想了一个0-98的均匀分布$M_a$
B想了一个均值45方差30的正态分布$M_b$
虽然这两个拟合看着都很不靠谱，但是这两个分布哪个更好一点呢？

K-L散度就可以解决这个问题

所谓拟合一个分布，其实就是得到一系列离散或连续的概率值$Q(x_i)$
通过观测数据/训练数据我们本身也可以得到其分布$P(x_i)$
K-L散度定义如下：
$$
\begin{alignedat}{2}
D_{KL}(P||Q)&=\sum p(x_i)(logp(x_i)-logq(x_i))\\\\
&=E[logp(x_i)-logq(x_i)]\\\\
&=\sum p(x_i)log\frac{p(x_i)}{q(x_i)}
\end{alignedat}$$
显然，K-L散度，是指**两个分布在同一值下对数概率之差的期望**
P，Q两分布越接近，KL散度越小
当两分布相等时(所有对应概率相等)，KL散度=0
**另外需注意，KL散度不对称即：**
$$D_{KL}(P||Q) \neq D_{KL}(Q||P)$$

##### 4.2 交叉熵(Cross Entropy)
由KL散度公式可以推算
$$
\begin{alignedat}{2}
D_{KL}(P||Q)&=\sum p(x_i)(logp(x_i)-logq(x_i))\\
&=\sum p(x_i)logp(x_i)-\sum p(x_i)logq(x_i)\\
&=-H(P)-\sum p(x_i)logq(x_i)\\\\
-\sum p(x_i)logq(x_i)&=H(P)+D_{KL}(P||Q)
\end{alignedat}$$
其中
$$H(P,Q)=-\sum p(x_i)logq(x_i)$$
就是PQ的交叉熵。
因为常常我们观测数据或训练数据的熵时固定的，所以KL散度和交叉熵只差固定值，所以，**可以用交叉熵代替KL散度来评估两个分布或某一个分部与训练数据的差异。**

##### 4.3 交叉熵损失函数

**1，对于一个二分类任务**
一个样本其交叉熵损失就是
$$L=-ylogp-(1-y)log(1-p)$$
其中
$y\in(0,1)$
$p=f(x)=p(y=1|x)$
则总体损失就是所有样本sum

**2，对于一个多分类任务**
一个样本的交叉熵损失就是
$$L=-\sum_{c=1}^{M}y_c logp_c$$
其中
$y_c \in (0,1)$ 表示样本是否属于c类
M 即类别c的数量
$p_c$ 即样本是c类的概率
则总体损失就是所有样本sum
由此可见，对于多样本交叉熵损失函数，其模型需要预测出样本属于各个类别的概率值

#### 5，最大化似然估计与对数损失(Maximum Likelihood Estimation & Logistic Loss)
#### 5.1 最大似然估计
最大似然估计的核心思想是：观测数据D之所以被观测到，是因为数据D出现的概率本身就高。想法很朴素。
似然函数如下：
$$
\begin{alignedat}{2}
l(\theta)&=p(x_1,x_2..x_n|\theta)\\\\
&=\prod_{i=1}^{n}p(x_i|\theta)
\end{alignedat}$$
最大似然估计即在当前数据下，求解令$l(\theta)$最大的$\theta$
例如估计抛硬币正面朝上的概率$p=\theta$
正面$p(ZM)=p=\theta$
反面$p(FM)=1-p=1-\theta$

**只抛1次，正面朝上**
$l(\theta)=p(ZM|\theta)=\theta$
则最大化$l=1$
即$\theta=1=p$

**抛10次，正6反4**
$l(\theta)=\sum p(x_i|\theta)=\theta^6(1-\theta)^4$
$\frac{\partial l(\theta)}{\partial \theta}=\theta^5(1-\theta)^3(6(1-\theta)-4\theta)$
求最大值，令导数=0得
$\theta=0.6$即p=0.6

**抛10000次，正4901反5099**
$\frac{\partial l(\theta)}{\partial \theta}=\theta^{4900}(1-\theta)^{5098}(4901(1-\theta)-5099\theta)$
得$\theta=0.4901$时，$l(\theta)$取得最大值

##### 5.2 最大似然函数做目标函数-对数损失 
例如在逻辑回归LR推导中，由于逻辑回归会估计样本是正样本的概率$p(y=1|x)=h(x)=sigmoid(wx)$，
**这里$y\in(0,1)$**
则对于一个样本(x,y)，其似然函数
$l=p(y=1|x)^y(1-p(y=1|x))^{(1-y)}$
对所有训练数据:
$L=\prod l$
又因为最大化L等价于最大化$log(L)$等价于最小化$-log(L)$
因为对数具有相乘变相加的特性
$$log(ab)=log(a)+log(b)$$
所以求解令-logL最小的参数(对于逻辑回归即是w)即可
$$-log(L)=\sum(-ylogp-(1-y)log(1-p))$$
这不正是**交叉熵损失函数吗？**
$-logL$ 也被叫做**对数损失。** 
**所以对数损失函数和交叉熵损失函数是等价的**
交叉熵损失函数，从熵的角度度量模型差异
对数函数，从统计估计的角度度量模型拟合
##### 5.3 对数损失的进一步扩展
假如我们令$y\in(-1,1)$ 即负类标签由0变为-1
$p(y=1|x)=sigmoid(f(x))=\frac{1}{1+exp(-f(x))}$
$p(y=-1|x)=1-sigmoid(f(x))=\frac{1}{1+exp(f(x))}$
$f(x)=wx$
则有
$p(y|x)=\frac{1}{1+exp(-yf(x))}$
则
$L=\prod p(y|x)=\prod \frac{1}{1+exp(-yf(x))}$

$-logL=\sum(1+exp(-yf(x)))$
这也是**对数损失** 只不过此时标签$y\in(1,-1)$ 

#### 6，指数损失(Exponential Loss)
$$L=exp(-y_if(x_i))$$
应用于加法模型Adaboost中。
因为指数具有指数相加等于数相乘的性质：
$$e^{a+b}=e^ae^b$$
所以，对于加法模型
$$f_t(x)=f_{t-1}+G_m(x)$$
有
$$\begin{alignedat}{2}
L&=exp(-y_if_t(x))\\
&=exp(-y_if_{t-1}(x))exp(-y_iG_m(x))\\
&=w*exp(-y_iG_m(x))
\end{alignedat}$$

#### 7,均方误差(Mean Square Error MSE)
$$L=\frac{1}{N}\sum_{i=1}^{N}(y-f(x))^2$$
#### 8,平方绝对误差(Mean Absolute Error MAE)
$$L=\frac{1}{N}\sum_{i=1}^{N}|y-f(x)|$$
##### 8.1 MSE VS MAE
1，MSE全程可导，MAE在0处不可导

2，MAE更鲁棒：
两者差别以$|y-f(x)|=1$为分界线
0~1之间时，MSE<<MAE
大于1时，MSE>>MAE
而当数据有异常值时，往往$|y-f(x)|>>1$
此时MSE>>MAE
这表示，使用MSE对异常值会更加敏感，而算法为了降低MSE，就会使模型过度偏向异常值。这种现象在MAE上就会减轻很多。所以可以说MAE相对于MSE更鲁棒一些。

**另一个解释是**，当我们用一个定值去拟合一列数时，MSE拟合的是数据的平均数，而MAE拟合的是数据的中位数。
$$
\begin{alignedat}{2}
MSE&=\frac{1}{N}\sum_{i=1}^{N}(x_i-\theta)^2\\
\frac{\partial MSE}{\partial \theta}&=\frac{2}{N}\sum_{i=1}^{N}(\theta-x_i)=0\\
\theta&=\frac{1}{N}\sum_{i=1}^{N}x_i\\
\end{alignedat}$$
**所以MSE拟合的是平均数**
对于MAE
我们对x从小到大排序
$$
\begin{alignedat}{2}
MAE&=\frac{1}{N}\sum_{i=1}^{N}|x_i-\theta|\\
&=\frac{1}{N}(\sum_{i=1}^{k}(\theta-x_i)+\sum_{i=k+1}^{N}(x_i-\theta) ) \\
\theta& \in [x_{k-1},x_{k+1}]\\\\
\frac{\partial MAE}{\partial \theta}&=\frac{1}{N}(\sum_{i=1}^{k}(1)+\sum_{i=k+1}^{N}(-1))=0\\
k&=\frac{N}{2}\\
\theta& \in [x_{N/2-1},x_{N/2+1}]\\
\end{alignedat}$$
**所以MAE拟合的是一个区间，这个区间通常可以取中位数替代。**
显然中位数对异常值是不敏感的，而平均值则会敏感。这提示我们对于不同的数据，需要选择不同的损失。就想在预测全国人均收入问题上，由于大部分财富集中在很小一部分人中，这些数据就相当于异常值，所以中位数更能代表人均收入水平。

3，对于梯度下降法来说，MSE梯度随着越接近最优点而越来越小，相当于一开始迈大步快速接近极值，后面迈小步精确靠近极值。而MAE的导数一直为1不变，使得它在靠近极值时容易一步跨过。

#### 9，Huber损失函数
$$L_{\delta}(y,f(x))=\begin{cases}\frac{1}{2}(y-f(x))^2,\text{ if }|y-f(x)|\leq \delta \\  \delta|y-f(x)| -\frac{1}{2}\delta^2, \text{ if }|y-f(x)|> \delta  \end{cases}$$
这个函数是全局可导的，他是MSE与MAE的结合，拥有两者的优点。通过$\delta$来调节MSE和MAE的占比。

#### 10，分位数损失函数(Quantile Loss)
要理解分位数损失函数，首先要理解MAE，MAE是拟合中位数的损失函数(即离差绝对值的期望在中位数处取得最低)
而中位数就是分位数的一种。

另外，我们考虑一种回归问题。以一元回归为例，在众多数据中，我们可以拟合一条曲线，这条曲线告诉我们对于某个x可能的拟合结果y=f(x)。即使真实y不是f(x)，也应该在f(x)附近，此时，我们的预测是一个点。
但是，我们如果想要获取一个y的范围呢？即对于x，我想知道y的大致范围。因为很多时候，我们不需要精确值，反而更需要一个范围值。此时，我们需要预测一个线段。
那怎么做呢？
其实如果我们能分别获得y的0.1分位数与0.9分位数的拟合，那么两者之间的部分就是我们需要的，它预测了y的80%的取值范围$f_{0.1}(x)-f_{0.9}(x)$
此时，就需要优化分位数损失函数：
$$L_{\gamma}(y,f(x))=\sum_{i\in \left \{  i|y_i<f(x_i)\right \}}(1-\gamma)|y_i-f(x_i)|+\sum_{i\in \left \{  i|y_i \geq f(x_i)\right \}}\gamma|y_i-f(x_i)|$$
即，该损失函数对预测值大于真实值和小于真实值得惩罚是不一样的。
当$\gamma=0.5$时，该损失等价于MAE
当$\gamma>0.5$时，该损失对预测值小于真实值的情况惩罚更大
当$\gamma<0.5$时，该损失对预测值大于真实值的情况惩罚更大

#### 11 总结
本节讲述了常见的损失函数。
损失函数大致分为应用于分类的和应用于回归的。
从7均方误差之后，基本上都用于回归。

**不同的模型会有自己比较适合的损失函数**

**回归问题**的损失函数，往往存在$y-f(x)$的部分，我们称之为残差
$$\varepsilon = y-f(x) \tag{11.1}$$ 回归损失函数往往围绕残差构建。

**分类问题**，就二分类(多分类往往建立在二分类的基础上)而言，标签y往往存在两种形式：
$y\in(0,1) \tag{11.2}$
or
$y\in(1,-1) \tag{11.3}$
对于11.2的情况，模型往往倾向于把预测值转换成概率$p=P(y=1|x)=f(x)$
从而使用交叉熵损失log损失，这些损失函数都存在$ylogf(x)$(或$f(x)^y$)部分。这样的模型有LR，神经网络
对于11.3的情况，模型往往倾向于构建分类超平面$f(x)=wx+b=0$，再通过$sign(f(x))$判断标签，例如感知机，SVM。又如集成学习Adaboost，其模型$f_t(x)=f_{t-1}(x)+\alpha G_t(x)$,而标签也是由$sign(f_t(x))$来决定，这几个模型的损失函数往往存在$yf(x)$的部分。
可以看到分类的损失函数，常围绕$yf(x)$来构建
当然，有的模型对于标签形式是不敏感的，例如k近邻，决策树等,因为这些模型没有把y用于损失计算，其损失函数也会比较不同。

**不同的损失函数也有不同的特性**

这些特性会有针对性的优化我们的某些需求。
MAE相比于MSE有较强的鲁棒性
分位数损失组合可以获得预测值的范围




#### 参考
[KL散度](https://www.jianshu.com/p/43318a3dc715)
[损失函数](https://zhuanlan.zhihu.com/p/39239829)
[Regression Loss Functions](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)
[Regression Loss Functions-翻译](https://yq.aliyun.com/articles/602858?utm_content=m_1000002415)
[分位数回归](https://blog.csdn.net/longgb123/article/details/85031163)
《统计学习方法》-李航