---
title: 古早博文-机器学习-正则化
date: 2017-12-31 19:57:28
categories: 
    - 技术
    - 机器学习
tags:
    - 机器学习
mathjax: true
---
#### 正则化
正则的目的，是为了减小**过拟合。**
正则可以减小过拟合？
首先，过拟合是因为过分拟合训练数据，使得模型过于复杂，模型过于复杂往往表示模型参数过多。
![正则1.jpg](/image/image-zhengze1.png)

如上图
我们以多项式回归模型拟合一组数据
红色曲线是拟合函数
$$h(x)=w^0+w_1x+w_2x^2+...+w_Mx^M$$
四个子图演示了回归模型从欠拟合到过拟合的变化。
图4可以帮助我们更好的理解过拟合，为了完全拟合训练数据，过分地调整了模型，使得模型本身复杂而曲折，**稍微偏离训练数据点就会带来很大的震荡(这也是高variance的一个原因)**，而且，**为了使曲线弯曲到如此程度，其导数也应该很大，这就导致参数w会很大**，否则难以带来函数剧烈的弯曲。这也告诉我们，好的模型应该是尽量平滑的，好的分类超平面也应该是尽量平滑的，带有剧烈曲折的拟合，往往会导致过拟合。

我们发现，过拟合时，往往代表参数过多，模型曲折往往代表参数过大。那我们就可以限定参数w的规模。这就是正则化。
正则通常是在损失函数后面加上正则项。如下：
$$min \sum_{i=1}^{N}L(y_i,h(x_i))+\alpha r(w)$$
$r(w)=\sum_{j=1}^{M}|w_j|$  
即为 L1正则
$r(w)=\sum_{j=1}^{M}|w_j|^2$ 
即为L2正则
常用的就是这两种正则项
$\alpha>=0$为正则化系数，其值越大，越不易过拟合，模型越简单。 
#### L1正则与L2正则
两者都会校正过拟合，虽然看起来差不多，但往往会体现出巨大的差异。
**L2正则会使w都变小而趋近于0**
**L1正则会更容易令部分w=0，从而使模型变稀疏，可以起到特征选择的作用**
**注意L2正则连续可导，L1在0处不可导。**
就$y=|x|$与$y=|x|^2=x^2$两个函数来看，其图像一个呈V型一个呈U型。随着$\alpha$的增大，V越来越窄而尖锐，U越来越窄而底部稍缓，远没有V尖锐。
一个很重要的问题是为什么L1正则会更容易让w=0？
**直观解释**
[L1正则直观解释](https://www.zhihu.com/question/37096933)
我们考虑
$$min \sum_{i=1}^{N}L(y_i,h(x_i))+\alpha |w|$$
$$min \sum_{i=1}^{N}L(y_i,h(x_i))+\alpha w^2$$
L1可用来筛选特征，是因为用L1更容易让某些w变为0，即他可以让某些w=0的时候，整体损失最低，即最低点。

所谓最优的wi，即上述曲线在w=wi处取得最小值或极小值。而欲让模型稀疏，即特征变少就要尽可能令wi=0
式子前部分的L是可导的凸函数，而L2也是连续可导，即式子2是连续可导的，若要在w=0处取得最小值则式子2的在0处的导数也应该=0
即$\frac{\partial L}{\partial w}| _{w=0}=0$ 

这个要求相对苛刻。
对于L1正则，在w=0处不可导，我们只要在w=0处对式子1取得最小值即可。
则式子1 $L+\alpha |w|$ 就像是除了w=0处，其他地方均对L拉高。所以只要 $\alpha$增大到一定程度，基本都可以让L在其他地方的值都高过w=0处的值。
你可能会疑问，L2也会有同样的效果。但L1更快。
因为在w=0~1处（往往w也是在这个范围）在这里L2<<L1,且越接近0，L2越远小于L1，这就导致需要更大的 $\alpha$ (可能就是平方)才能达到和L1相同的效果。
但我们也知道，过大的$\alpha$会令模型欠拟合，所以一味增大也是不可取的。这样对比来看，L1是会比L2更容易获得稀疏的模型。


两种 regularization 能不能把最优的 x 变成 0，取决于原先的费用函数在 0 点处的导数。如果本来导数不为 0，那么施加 L2 regularization 后导数依然不为 0，最优的 x 也不会变成 0。而施加 L1 regularization 时，只要 regularization 项的系数 C 大于原先费用函数在 0 点处的导数的绝对值，x = 0 就会变成一个极小值点。上面只分析了一个参数 x。事实上 L1 regularization 会使得许多参数的最优值变成 0，这样模型就稀疏了。
[链接](https://www.zhihu.com/question/37096933/answer/70426653)


**最优化解释**
[L1正则数学解释](https://zhuanlan.zhihu.com/p/31458541)
数学解释需要理解的根本是把一个无约束的最优化问题转变成了带约束的最优化问题。
$$min \sum_{i=1}^{N}L(y_i,h(x_i))+\alpha r(w)$$
是无约束的，即w可以取任意值
而这个问题可以转化为带约束的问题：
$$min \sum_{i=1}^{N}L(y_i,h(x_i))$$
$$s.t.: r(w)<=\eta$$
而这个转化直接可以从图像上表示出来：
![正则2.jpg](/image/image-zhengze2.png)
蓝色同心圆代表L的等高线，橘黄区域代表符合约束条件的w区域。可见菱形比圆形更容易相交于轴线上，而轴线上代表有一个w分量=0
由此可知，L0.5正则会更容易获得稀疏解，因为L0.5比菱形更内凹



#### 参考
[csdn](https://blog.csdn.net/Nietzsche2015/article/details/43450835)
[正则化](https://www.cnblogs.com/jianxinzhou/p/4083921.html)
[正则-知乎](https://www.zhihu.com/question/20924039)


