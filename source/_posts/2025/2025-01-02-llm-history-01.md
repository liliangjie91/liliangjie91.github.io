---
title: 大语言模型演进史-01-概述
mathjax: true
date: 2025-01-02 13:09:01
categories:
    - 技术
    - AI大模型
tags:
    - 大模型演进
    - AI模型
    - NLP
description: ChatGPT的出现是AI发展的重要里程碑，使得大模型正式破圈。AI大模型（亦称：大模型、大语言模型、AI模型）有较为清晰的脉络，本文主要概述大模型的演进历史，从Word2Vec到GPT。
---
大模型即大语言模型，本质是一个语言模型，但因为参数量巨大(百亿千亿级别)，数据量巨大(数千亿-万亿个token)，
训练量巨大($10^{24}$左右次浮点运算，一块4090需要算**400年**)，故称为**大**语言模型。  
我们对大模型最直观的印象就是：可进行**有效的交谈**。即模型似乎理解我的话，也能说出对应的有意义的回答。  
在语言模型角度，在他面前有无数句话（无数选择），但他选择了最适合当下状态的那一句（让“标准答案”的概率最高）。  
# 什么是语言模型
语言模型通俗讲就是**评价一句话存在的概率**的一个模型。一句话通常由一组词组成，故模型抽象成数学表达即：
$$P(s)=P(w_1, w_2, \dots, w_n)$$
那知道这个$P(s)$有什么用？
- 在语音识别，翻译等任务上，这个概率可以作为指导，选出更符合正常人类表达的结果
- 当模型能够判断时，他就有可以"说"出像人类的话。应用在语言生成，对话任务中。
- 在此基础上其他应用如词向量等  

**如何理解“一句话存在的概率P”**。假设一本书有10000句话，“今天天气不错”出现了10次，那么  
$P(今天天气不错)=10/10000=0.001$  
通常一个语言模型的训练数据集包括书籍和网页等，那么可以通过统计这个数据集中每句话的频率继而计算概率。
但这样有个问题，即无论收集多少数据，总会有统计不到的句子。  
所以可以不统计句子，而统计词，**通过词频率推导句子的概率**。这样有更强的适应性。
# 语言模型公式推导
针对一句话$(w_1, w_2)$根据**条件概率公式**可计算其概率 

$P(w_1, w_2)=p(w_2|w_1) * p(w_1)$ 以此类推：  

{% raw %}
$$
\begin{alignedat}{2}
P(w_1, w_2,w_3) &= p(w_3|w_1,w_2) * p(w_1,w_2)\\
&= p(w_3|w_1,w_2) * p(w_2|w_1) * p(w_1)  
\end{alignedat}$$
{% endraw %}

{% raw %}
$$
\begin{alignedat}{2}
P(w_1, w_2,w_3,w_4) &=p(w_4|w_1,w_2,w_3) * p(w_1,w_2,w_3)\\
&=p(w_4|w_1,w_2,w_3) * p(w_3|w_1,w_2) * p(w_2|w_1) * p(w_1)
\end{alignedat}$$
{% endraw %}

对于任意句子
{% raw %}
$$
\begin{alignedat}{2}
P(w_1, w_2,w_3,...,w_n) &= p(w_1) * p(w_2|w_1) * p(w_3|w_1^2) * p(w_4|w_1^3) * ... * p(w_n|w_1^{n-1}) \\
&=  \prod_{i=1}^n p(w_i | w_{1}^{i-1})\\
&其中w_{1}^{i} = (w_1,w_2,...,w_i)
\end{alignedat}$$
{% endraw %}

例如
{% raw %}
$$
\begin{alignedat}{2}
P(今天天气不错啊) &= p(今天) * p(天气|今天) * p(不错|今天,天气) * p(啊|今天,天气,不错)
\end{alignedat}$$
{% endraw %}
通过上述公式，对于一个固定训练数据集，假设去重后有n个词，如果要计算任意一句话的概率，就需要计算：
1. 每个词的概率$p(w_i)$，需要计算n个概率值
2. 每个词相对于任意另1个词的条件概率$p(w_i|w_j)$，需要计算$C_{n}^{2}个 $
3. 每个词相对于任意另2个词的条件概率$p(w_i|w_{j_1},w_{j_2})$，需要计算$C_{n}^{3}个 $
4. 每个词相对于任意另n-1个词的条件概率$p(w_i|w_{j_1},w_{j_2},...,w_{j_{n-1}})$ ，需要计算$C_{n}^{n-1}个 $ 

我们发现，计算量是非常庞大的，因为一般n是比较大的例如万级十万级

# 大语言模型演进
## 统计语言模型 - N-gram
一个简单的方法就是不计算那么多条件概率，不必计算一个词前10个词的条件概率，计算前几个就够了。因为从自然语言角度理解，看到前3个词，我们大概就能猜出下一个词。  
所谓N-gram就是几个词可以组成最小的独立单元：n元组。
### 1-gram：unigram
1-gram即每个词都是最小独立单元，即每个词之间都是相互独立  
则各个条件概率都简化成了每个词的概率：$p(w_i|w_1^k)=p(w_i)$,则句子概率：
{% raw %}
$$
\begin{alignedat}{2}
P(w_1, w_2,w_3,...,w_n) &= p(w_1) * p(w_2) * p(w_3) * ... * p(w_n) \\
&=  \prod_{i=1}^n p(w_i)\\
\end{alignedat}$$
{% endraw %}
### 2-gram：bigram
2-gram即每2个词都是最小独立单元，即每个词只和前1个词有关系，则句子概率：  
{% raw %}
$$
\begin{alignedat}{2}
P(w_1, w_2,w_3,...,w_n) &= p(w_1) * p(w_2|w_1) * p(w_3|w_2) * ... * p(w_n|w_{n-1}) \\
&=  \prod_{i=1}^n p(w_i|w_{i-1})\\
\end{alignedat}$$
{% endraw %}
### N-gram
N-gram即每N个词都是最小独立单元，即每个词只和前N-1个词有关系，则句子概率：  
{% raw %}
$$
\begin{alignedat}{2}
P(w_1, w_2,w_3,...,w_n) &= p(w_1) * p(w_2|w_1) * p(w_3|w_1,w_2) * ... * p(w_{n}|w_{n-N+1}^{n-1}) \\
&=  \prod_{i=1}^n p(w_i|w_{i-N+1}^{i-1})\\
\end{alignedat}$$
{% endraw %}
其中
$$p(w_i|w_{i-N+1}^{i-1})=\frac{count((w_{i-N+1},w_{i-N+2},...w_{i-1},w_{i}))}{count((w_{i-N+1},w_{i-N+2},...w_{i-1}))}$$

我们发现，对于N-gram模型，我们只需计算几级条件概率，计算条件概率的过程，就是统计各种频率后计算。所以这同样属于**统计语言模型**。

## 神经网络语言模型
神经网络语言模型即利用神经网络建模语言模型  
采用机器学习的思想，把语言建模问题抽象成预测问题，例如基于前N个词预测当前词。    
神经网络有强大的**拟合能力**，同时为了适配神经网络，**词嵌入**成为必不可少的步骤，所以同时完成了自然语言到数学空间的映射。  
受限于计算机算力，在早期神经网络语言模型发展缓慢，但在算力与模型的不断发展下，结果大家也已经看到了。

---
### 神经语言模型的鼻祖 - NNLM - 2003
[A Neural Probabilistic Language Model - 12001引用-截至20250101](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)   
2003年，Bengio发表论文A Neural Probabilistic Language Model开启了神经网络语言模型(NNLM)领域，论文首次提出使用神经网络构建语言模型，其基本结构沿用至今。  
NNLM沿用N-gram思想，即前N个词预测下一词的模式。当模型训练完成之后  
1. 其前向过程可以做下一词预测。   
2. 对每个词做了分布式表征`a distributed representation for each word`即词向量`Word Embedding`

#### 模型架构

<img src="/image/2025/image-NNLM01.png" width=400 height=300 />

模型为一个简单的3层MLP  
**输入层**  $C\in \mathbb{R}^{ N \times m}$,N为词表大小，m为词向量维度。  
$C(i)$表示第$i$个词的向量
$$x = concat(C(w_{t-n+1}),C(w_{t-n+2}),...,C(w_{t-1}))$$
**隐藏层**  h个神经元，权重矩阵$H\in \mathbb{R}^{ h \times (n-1)m}$  
激活函数为$tanh$
$$hidden = tanh(Hx + d)$$
**输出层**  N个神经元(对应词表中的N个词)权重矩阵$U\in \mathbb{R}^{N \times h}$,还有虚线部分权重  $W\in \mathbb{R}^{N \times (n-1)m}$  
$$y = Utanh(Hx + d) + Wx + b$$
最后对y做softmax

#### 评价
- 从现在的角度看，神经网络语言模型性能远超统计语言模型，
    - 他能捕捉长距离的词关系（n-gram只能捕捉n个词的距离）；
    - 复杂的模型显然更能捕捉文本的语义外在表现就是效果更好；
    - NNLM最大的贡献在于把**实现了自然语言到数学空间的映射**，这拓宽了它的使用场景：除了计算条件概率之外，其产出的向量使得文本可以在语义层面做计算例如相似度，聚类，分类等
- 但从当时来看，算力依旧是限制其发展的重要原因。模型每次训练都要计算所有词的概率，需要很大的计算量(这也为Word2vec的优化提供了空间)

---
### 词嵌入思想的布道者 - Word2Vec - 2013
2013年，即NNLM提出的10年后，Tomas Mikolov 先后发表  
[Efficient Estimation of Word Representations in Vector Space - 45468引用-截至20250101](https://arxiv.org/abs/1301.3781)  
[Distributed Representations of Words and Phrases and their Compositionality - 46189引用-截至20250101](https://arxiv.org/abs/1310.4546)  
它引入了 **Skip-gram 和 CBOW** 两种高效训练算法，并针对NNLM计算量消耗最大的最后一步提出了**层次softmax和negative sample负样本采样**这两种优化方法
它的**核心功能就是得到word embedding词向量**，用于下游任务。它是一个快速高效的词嵌入工具。人们也逐渐意识到词向量的用处。

    所谓词嵌入，即对文本序列中的一个个词映射到高维空间。  
    其实不一定是文本序列，也可以是用户的点击序列，购买序列。  
    如果说对文本序列中的词做嵌入，是对整体人类语言的模拟。    
    那么，对用户点击序列中item-id做嵌入，是对整体用户点击习惯的模拟。  
    所以在推荐系统领域，基于word2vec启发而来的item2vec也是一种快速高效的召回算法。  

2014年，Mikolov趁热打铁推出[Distributed Representations of Sentences and Documents](https://arxiv.org/abs/1405.4053)即Doc2vec，文档嵌入。  
此算法可以把句子或文档映射成一个向量。从而可以更容易地实现相似文档查找，文档分类等任务  
word2vec，doc2vec的出现，加速了自然语言**向量化**进程，也更容易接入下游任务

---
### 注意力机制的催化剂 - Seq2Seq - 2014 

[Sequence to Sequence Learning with Neural Networks - 27886引用-截至20250101](https://arxiv.org/abs/1409.3215v3)  

Seq2Seq模型使用了**编码器解码器结构(Encoder-Decoder)**。这种结构主要应对翻译这种把**一个序列转换为另一个序列**的任务。  
Seq2Seq的解码器编码器主要是RNN模型(LSTM，GRU)。RNN(循环神经网络)在结构上适合对序列建模，于是自然应用到了自然语言处理。  
**CNN处理图像，RNN处理语言**。人们仿佛找到了处理这两种模态数据的终极武器，直到几年后，**Transformer**问世。  

#### 模型架构
Seq2Seq在结构上是2个RNN模型的拼接，前面的RNN(编码器)编码输入，得到整个结构的中间状态。后面的RNN(解码器)接受中间状态继而计算输出。
<img src="/image/2025/image-seq2seq01.png" width=400 height=200 />  

那这和1个RNN结构有什么不同呢？
 - 一个最主要优势是，Seq2Seq模型便于处理输入输出**不等长的数据**，在翻译中输入和输出长度往往不同。而相比之下，单个RNN在处理这种数据时，性能可能差一点。
 - Seq2Seq为解码器如何利用中间状态提供了很大探索空间，为**注意力机制**的发展提供了结构基础。
 - 翻译任务中，源语言和目的语言在映射到高维空间时，其内在的分布是不同的，将两个空间分别交由两个RNN处理，是一种**解耦**的处理方式，符合直觉。

#### Attention机制
[Neural Machine Translation by Jointly Learning to Align and Translate - 37257引用-截至20250101](https://arxiv.org/abs/1409.0473)  
就在同年，把注意力机制引入Seq2Seq的工作就有人做了-Bengio
<img src="/image/2025/image-seq2seq02.png" width=400 height=250 />

解码器在利用中间状态时，不再只是用编码器的最后一个输出，而是把所有输出做一个加权求和
$$c_i = \sum_j \alpha_{ij} \cdot h_j^{encoder}$$
其中$\alpha_{ij}=h_i^{decoder} * h_j^{encoder}$此处为向量点乘，得到$\alpha_{ij}$为标量  

$h_j^{encoder}, h_i^{decoder}$分别表示编码器解码器的第$j,i$个中间向量。  

所谓注意力机制，即解码器得到第一个中间状态，然后用这个状态向量和所有编码器中间向量计算相似度(点乘)，
然后依据相似度把编码器的所有中间状态组合起来得到一个新的和当前解码器中间状态相关的状态，然后用这个状态向量替代解码器的中间状态向下计算...  
注意力机制像是香水师调香水，不同香味取不同量组合在一起。

---
### AI时代的奠基之作 - Transformer - 2017

[Attention Is All You Need - 147563引用-截至20250101](https://arxiv.org/abs/1706.03762)  
好不好，看引用量即可。
文章标题已经指出了全文重点：**注意力机制就是终极答案**，模型使用了3中注意力方式：self-Attention自注意力，cross-Attention交叉注意力，masked-Attention掩码自注意力
#### 论文被引数量变化

<style>
  table {
    width: 30%;
    font-size: 12px;
  }
  th, td {
    padding: 2px;
    text-align: center;
  }
</style>

|时间|被引次数|出处|
|--|--|--|
|2018-03|116|[here](https://cyeninesky3.medium.com/attention-is-all-you-need-%E5%9F%BA%E6%96%BC%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6%E7%9A%84%E6%A9%9F%E5%99%A8%E7%BF%BB%E8%AD%AF%E6%A8%A1%E5%9E%8B-dcc12d251449)|
|2018-12|934|[here](https://www.jiqizhixin.com/articles/2018-12-19-10)|
|2020-05|6100|[here](https://blog.csdn.net/qq_27075943/article/details/106244465)|
|2021-04|19000+|[here](https://cloud.tencent.com/developer/article/1816846)|
|2022-05|41000+|[here](https://www.thepaper.cn/newsDetail_forward_18034683)|
|2023-06|77000+|[here](https://36kr.com/p/2299690077875202)|
|2024-04|11万+|[here](https://www.odaily.news/post/5194149)|
|2025-01|14万+|本文时间|

RNN的结构确实善于处理序列数据，门控单元的设计(LSTM，GRU)也使得对长序列建模成为可能。  
但是RNN不能并行训练，从而减慢了训练速度。要提高训练并行度，就必须摒弃RNN的循环结构。
于是，新的结构被提出：Transformer 
#### 模型架构
Transformer依旧是一个Encoder-Decoder结构，论文中也是用于翻译任务。而在encoder，decoder结构中  
不是RNN模型，而是基于Self-Attention(自注意机制)的多头注意力模块。
<img src="/image/2025/image-transformer1.png" width=300 height=400 />

Transformer把注意力机制发挥到极致，整个模型几乎全由**注意力机制+简单前向神经网络**组成。  

#### RNN与Transformer
**自注意机制就是替代RNN的核心**：
RNN中，每个词都会接受前面词留下的信息(中间状态)用于后续运算，这是RNN对**序列**建模的核心手段。
Transformer中，每个词都是其他词的加权平均，权重就来自这个词和其他词的相似度。即我(词)不是我，周边人对我的“评价”才是我。
Transformer相比于RNN，一是可并行，二是词关系不会随距离衰减且一次性计算即可(但缺点是计算量随距离平方增加)。  

- 现在回头看，Transformer在神经网络语言模型建模中彻底释放了GPU的算力潜能，为大模型的发展打下了最坚实的基础。
在速度上，它适配GPU的并行计算。在建模上，它适配序列这种元素间存在相互关系的数据类型。

#### Seq2Seq与Transformer
Seq2Seq，Transformer都是Encoder-Decoder结构，都是在处理机器翻译任务时被提出的，属于NLP的一个分支任务。
但实际上稍作调整，这些模型就可以应用到其他任务，因为翻译任务本质是把一个序列映射到另一个序列。  
而大多数NLP任务，都可以抽象成这种形式。
例如文本分类：长序列输入对应极短输出；文本生成：短对长；智能问答：序列对序列；语音识别：序列对序列...  
在向量抽取方面，这些模型也都可以做到

---
### RNN最后的荣光 - ELMo - 2018
[Deep contextualized word representations - 16290引用-截至20250101](https://arxiv.org/abs/1802.05365)

文章标题Deep **contextualized** word representations-深度**语境化**的词表征。所谓语境化即每个词在不同的句子里，意义不同故而向量不同。  
Elmo的**主要功能就是提供词表征(词向量)**。因为在当时，词向量凭借对神经网络的无缝接入，应用十分广泛
**向量化就是自然语言对接各种nlp任务的接口**
他与Word2vec的不同就像上面说的，每个词会根据其所在的句子而有不同的向量表示。 

#### 模型架构
<div style="display: flex; justify-content: space-between;">
  <!-- <img src="/image/2025/image-Elmo1.png" width=400 height=300 style="width: 35%;" alt="Image 1"> -->
  <img src="/image/2025/image-Elmo2.png" width=400 height=300 style="width: 45%;" alt="Image 2">
</div>

这是Elmo的模型架构，由两个多层(L层)LSTM模型组合而成(紫色+绿色)，故而是一个双向语言模型。

#### ELMo向量获取
如上图红色框的ELMo即最终的Embeddings from Language Models，它由黄线中的embedding计算所得。
{% raw %}
$$R_k=\{x_k^{LM}, \overrightarrow{h_{k,j}^{LM}}, \overleftarrow{h_{k,j}^{LM}} |j=1, \cdots,L\}=\{h_{k,j}^{LM}|j=0,\cdots,L\}$$
{% endraw %}  
即初始词向量以及每层LSTM的输出状态，一共有$2L+1$个向量。
最终的ELMo由这些向量组合而来  
$$ELMo_k^{task}=E(R_k;\Theta^{task})=\gamma^{task} \sum_{j=0}^L s_j^{task} h_{k,j}^{LM}$$
对于每一个下游任务，有不同的组合参数

---
### NLP微调模型之王 - BERT - 2018

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - 123165引用-截至20250101](https://arxiv.org/abs/1810.04805)
2018年6月份GPT-1发布，使用了Transformer的Decoder部分  
2018年10月份BERT发布，使用了Transformer的Encoder部分  
而且BERT的模型大小直接对标GPT-1。`BERT-base was chosen to have the same model size as OpenAI GPT for comparison purposes` 1亿+参数。  
You know what im saying？  
BERT确实也成功了，从引用量就可以看出。  
本身BERT的性能也会比GPT-1好一点，同时又背靠Google，更何况Transformer本身也是Google推出的，更更何况2019年推出的GPT-2是闭源的。所以BERT取得如此成就没毛病。  

从标题可以看到
1. BERT模型是一个预训练模型，可以拿这个模型去做其他下游任务（微调）。
2. 是一个双向的语言模型。
3. 使用了Transformer

#### 模型架构
<img src="/image/2025/image-bert1.png" width=900 height=350 />

上图同时比较了BERT，GPT-1，ELMo  
可以看到，三者架构相似，都是多层结构(就像CNN的多层结构，都是要逐步提取输入内容的信息)。其中  
- BERT和GPT-1都使用了更先进的Transformer
- BERT和ELMo都是双向语言模型，即一个词会看其左右的词的信息（类似完形填空）。而GPT是单向，即词只能看到它前面的词（这更符合文本生成等任务）
- 图中还说BERT的双向模型更耦合，ELMo仅仅是两个模型拼接，而BERT是训练中即时双向的。
- BERT和GPT-1都是预训练模型，并且可以用微调的方式应用到下游任务。而ELMo是“feature-based”方法，即他是生成好向量之后应用到不同的下游任务，下游任务和模型是分离的。

实际上，BERT使用的是Transformer的Encoder部分，其自注意力机制本身就会看到每个词的前后词，这本就是Encoder的特性。而为了适应这个特性，BERT就设计了类似完形填空的Masked-LM任务。

#### 预训练与微调
NLP领域的预训练与微调来自计算机视觉(CV)领域的**迁移学习**思想。*例如在CV中用 ImageNet 训练的模型迁移到特定任务。*
- **预训练**即使用大量数据训练一个基础的模型，
- 然后在具体的任务上再用任务数据(相比预训练数据要少)用较小的学习率调整该模型或是调整该模型一小部分架构--即**微调**  
GPT，BERT，ELMo都是预训练模型，实际应用时，用对应的训练数据微调，以适应当前任务。
但在具体微调时，又有不同。尤其ELMo，主要是使用向量用于下游任务，不会对向量再做调整。
而在使用BERT和GPT时，通常会对所有参数都做微调。

<style>
  table {
    width: 50%;
    font-size: 12px;
  }
  th, td {
    padding: 2px;
    text-align: center;
  }
</style>

| 微调特性              | ELMo                 | BERT                  | GPT                   |
|-------------------|----------------------|-----------------------|-----------------------|
| **参数更新范围**  | 通常固定，部分微调   | 全参数微调            | 全参数微调            |
| **模型复杂度**    | 低                   | 中高                  | 中高                  |
| **下游适配性**    | 适合嵌入式应用       | 广泛适用性（理解任务）| 强于生成任务          |
| **数据需求**      | 较少                 | 较多                  | 较多（生成更敏感）     |
| **计算资源需求**  | 较低                 | 较高                  | 较高                  |

#### 文本理解 VS 文本生成

<style>
  table {
    width: 50%;
    font-size: 12px;
  }
  th, td {
    padding: 2px;
    text-align: center;
  }
</style>

| **特性**         | **文本理解任务**                   | **文本生成任务**                   |
|------------------|-----------------------------------|------------------------------------|
| **目标**         | 理解和分析已有文本内容             | 基于上下文生成新的文本内容          |
| **方式**         | 提取信息、分类、判断或匹配           | 逐词生成，预测后续内容              |
| **输入输出关系** | 输入完整文本，输出特定的结构化信息   | 输入部分文本，输出完整的新文本       |
|**举例**         | **分类**：情感分析、新闻分类     | **文本续写**：小说续写、邮件生成 |
|**举例**         | **标注**：命名实体识别（NER）、词性标注 | **翻译**：英语到法语翻译          |
|**举例**         | **问答**：阅读理解（SQuAD）      | **对话生成**：客服聊天机器人      |
|**举例**         | **文本匹配**：语义相似度判断      | **摘要生成**：长文自动摘要        |
|**举例**         | **推理**：自然语言推理（NLI）     | **创意写作**：诗歌、故事创作      |

当然两种任务的分类并不绝对，两者本身是相互联系的，文本生成离不开文本理解，文本理解最终也要生成结果。

#### BERT的优势
在chatGPT出现之前，BERT的使用量影响力都是巨大的。
由上表可以看出，BERT**双向语言模型**的特性使其更注重**文本理解**而非**文本生成**（GPT更为擅长）。
而且BERT对词向量的构建更加精准，毕竟左右都看一次和只向左看一次能得都更多信息。
在实际NLP任务中，文本理解任务比文本生成任务**更常见更实际**，同时大家更希望用一个**更好的词向量**。所以在工业界BERT自然使用较多。

---
### AI时代的开山之作 - GPT系列 - 2018-2020

[GPT-1 Improving Language Understanding by Generative Pre-Training - 12212引用-截至20250101](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  
[GPT-2 Language Models are Unsupervised Multitask Learners - 15084引用-截至20250101](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
[GPT-3 Language Models are Few-Shot Learners - 37949引用-截至20250101](https://arxiv.org/abs/2005.14165)

GPT全称Generative Pre-trained Transformers抑或是论文中的Generative Pre-Training。我认为前一个更好，更精确描述了论文主旨：
1. 使用了Transformer
2. 生成式模型
3. 预训练模型

GPT的目标定位就是处理**生成式任务**，
- Decoder使用的是Masked Self-Attention机制，**在训练时**就看不到预测词后面的信息，所以就擅长处理*只给前面不给后面*的任务
- Decoder具有自回归的特性（预测出一个词A，把A拼接到输入后再次预测出B，把B再拼接到输入...直到输出完成），**在预测时**就可以一个词一个词地生成  

所以Decoder与生成任务高度契合。
相比之下Encoder使用的是Self-Attention机制，在训练时可以看到前后信息，所以对词地理解就更全面。

#### 模型架构

<img src="/image/2025/image-gpt1.png" width=900 height=500 />

左侧是模型架构，右侧是在对下游各种任务做Fine-tuning时，输入数据的设计。

#### GPT-1-2-3发展：Zero-shot之路

GPT-1推出之后BERT继而问世，凭借更优秀的性能和大厂背书，BERT的声量远超GPT-1。作为GPT-1的作者，Alec Radford要如何做呢？
他当然要在**生成式模型**的路上继续走下去。

<style>
  table {
    width: 80%;
    font-size: 12px;
  }
  th, td {
    padding: 2px;
    text-align: center;
  }
</style>

| **方面**         | **GPT-1**                           | **GPT-2**                           | **GPT-3**                           |
|------------------|-------------------------------------|-------------------------------------|-------------------------------------|
| **参数量**       | 1.17 亿                            | 15 亿（最大版本）                   | 1750 亿                              |
| **模型深度**     | 12 层                              | 48 层                            | 96 层                               |
| **head数**       | 12                               | 25                             | 96                                |
| **隐藏维度**     | 768                                | 1600                               | 12288                              |
| **上下文窗口**   | 512                                | 1024                               | 2048                               |
| **词表大小**   | 4万                               | 5万                              | 5万                          |
| **数据集**       | BooksCorpus（7GB）                 | WebText（45GB）                     | 混合数据集（Common Crawl、书籍、维基百科等，570GB 过滤后） |
| **训练方法**     | 无监督预训练 + 监督微调            | 强化无监督预训练 + 零样本能力增强    | 强化零样本和小样本学习能力，基于Prompt任务设计 |
| **任务表现**     | 需要微调完成下游任务               | 提供一定的零样本任务解决能力         | 出色的零样本和小样本学习能力，多任务迁移效果显著 |
| **生成能力**     | 流畅性较强，但易生成重复或无意义文本 | 长文生成能力增强，逻辑更清晰、更连贯 | 生成质量显著提升，语义一致性强，风格和逻辑多样化 |
| **创新点**     | 基于 Transformer 的初步应用         | 零样本学习的初步展现                | Few-Shot Learning（小样本学习）的显著突破 |

**GPT-2对比GPT-1**
模型大了数据量大了但模型架构变化很小（layer-norm位置有所变化从$LayerNorm(X+atten(X))$变为$X+atten(LayerNorm(X))$）。
性能提高了，除此之外还需要提出一些创新点。   
GPT-2提出的创新点就是：预训练之后**不微调**，直接用于下游任务---**零样本学习能力**（Zero-shot learning）。  
他验证了随着模型变大数据变大，模型可以直接用于下游任务（进一步减少下游任务的训练成本）  

**零样本学习**  
如果模型不再微调，那么在应用到下游任务时，输入数据的格式就需要和预训练时输入数据一样-即一段自然语言文本-也即：**Prompt**
zero-shot输入：“翻译成中文，hello =>”  
one-shot输入：“翻译成中文，world => 世界，hello =>”
few-shot输入：“翻译成中文，world => 世界，model => 模型，learn => 学习，hello =>”

**GPT-3对比GPT-2**
GPT-2确实是创新的，不微调确实是很大的卖点。但是有效性差一些，即效果不够哇塞。GPT-3就是要解决有效性问题。
于是GPT-3来个大的，模型加大100倍数据加大10倍。结果也确实更好。在Few-Shot Learning（小样本学习）上有显著突破 

---
### 初代通用人工智能(AGI) - chatGPT - 2022

<img src="/image/2025/image-gpt-develop-line.png" width=900 height=250 />

在经过GPT1-2-3的发展之后，模型规模已经足够大，要解决的问题是：如何更好解决多样的NLP任务。

以下参看[此文](https://cloud.tencent.com/developer/article/2429820)

    OpenAI在GPT-3模型的基础上，通过两种主要途径进行了改进：代码数据训练和人类偏好对齐。
    1. 首先，为了解决GPT-3在编程和数学问题求解上的不足，OpenAI于2021年推出了**Codex模型**，该模型在GitHub代码数据上进行了微调，显著提升了解决复杂问题的能力。
    此外，通过开发一种对比方法训练文本和代码嵌入，进一步改善了相关任务的性能。
    这些工作促成了GPT-3.5模型的开发，表明在代码数据上的训练对提高模型的综合性能，尤其是代码能力具有重要作用。
    2. 其次，OpenAI自2017年起就开始了**人类偏好对齐**的研究，通过强化学习算法从人类标注的偏好数据中学习改进模型性能。
    2017年，OpenAI提出了**PPO算法**，成为后续人类对齐技术的标配。
    2022年，OpenAI推出了InstructGPT，正式建立了**基于人类反馈的强化学习算法RLHF**
    旨在改进GPT-3模型与人类对齐的能力，提高指令遵循能力，并缓解有害内容的生成，这对大语言模型的安全部署至关重要。
    3. OpenAI在其技术博客中描述了对齐研究的技术路线，并总结了三个有前景的研究方向：使用人类反馈训练人工智能系统、协助人类评估和进行对齐研究。
    通过这些增强技术，OpenAI将改进后的GPT模型命名为GPT-3.5，它不仅展现了更强的综合能力，也标志着OpenAI在大语言模型研究方面迈出了重要一步。

#### chatGPT 训练流程

<img src="/image/2025/image-gpt-instructgpt-pipline.png" width=800 height=600 />
<img src="/image/2025/image-gpt-chatgpt-pipline.jpg" width=800 height=400 />

上图出自[instruct-GPT论文](https://arxiv.org/abs/2203.02155)  
下图出自[OpenAI前创始成员Andrej Karpathy的演讲](https://www.youtube.com/watch?v=bZQun8Y4L2A)

大体揭示了当前大模型训练的步骤：
1. 预训练
2. 监督微调
3. 训练奖励模型RM
4. 人类对齐强化学习(RLHF)

第一步其实在GPT-3就已经做了。当时的模型已经具有了很强的**few-shot**能力。所以后面的事就是把few-shot**推向zero-shot** 外加 提高**安全性**
**zero-shot**
zero-shot可以通过监督微调提高，即人工生成各种prompt+答案作为训练数据。但人工生成数据毕竟有限，也需要考虑到人工成本。所以这一步训练数据不会太多。
也一步也可以认为是在激发模型的**涌现能力**，向一个扳机/trigger。
**安全性**
现在模型能说会道了，但需要一个把门的。即需要让模型知道什么样的回答**有用并且安全的**，这个有用安全是相对其用户——人类来说。即模型的输出要符合**人类的普遍价值观**。
那如何让模型知道这样**相对抽象的标准**呢：第一步让人对模型的输出打分。第二步让模型根据这个打分去修正。这也就是RM模型和RLHF的作用。
  - RM模型：让人对模型输出打分太慢了，所以就专门训练一个打分模型。先采集一些人类打分数据，然后训练RM。这个RM就承载了人类的价值观。
  - RLHF：RL即强化学习。HF-human feedback即RM模型评分。训练RM的目的，最终是为了指导大模型的训练方向，具体的方法即RL

## 回顾语言模型发展

<img src="/image/2025/image-nlp-model-history.png" width=600 height=200 />
NLP的四个范式：

P1. 非神经网络时代的完全监督学习 （Fully Supervised Learning, Non-Neural Network）
P2. 基于神经网络的完全监督学习 (Fully Supervised Learning, Neural Network)
P3. 预训练，精调范式 (Pre-train, Fine-tune)
P4. 预训练，提示，预测范式（Pre-train, Prompt, Predict）

# 总结

本文粗略总结了大模型出现前NLP语言模型领域的一些重要模型，主要篇幅是神经网络语言模型。
其实从时间角度看，统计语言模型已经出现60，70年之久，而神经网络语言模型仅20余年，而且其主要发展都集中在近10年。所以在发展区线上看是一个典型的指数发展曲线。
而这背后离不开计算机算力的迅猛发展，尤其以Nvidia/英伟达为代表的GPU发展。
这里面最关键的，就是对抽象数据的数学化，无论是NLP的词嵌入，还是CV的卷积。都是找到一个模型，把抽象的内容，转化为一堆数字。在这个过程中，需要消耗巨大算力，也需要消耗巨量数据。
大模型的预训练其实是对世界知识的压缩。所谓世界知识，即互联网上的网页，书籍等人类的公开的共同知识集合。
但现在这个数据已经几乎被用尽了。而后续的互联网数据，其实有很多已经是AI生成的，而且会越来越多
所以这条曲线，还会如指数般发展下去吗？
尖端的技术可能会缓慢发展，但是技术的应用却会如雨后春笋。就行开辟了一片新土地，就会有新的植物在土地上生长。
依靠大模型以及GPU技术，机器人的发展将迈上一个新的台阶。