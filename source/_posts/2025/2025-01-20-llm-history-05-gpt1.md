---
title: 大语言模型演进史-05-GPT1
mathjax: true
date: 2025-01-20 21:20:21
categories:
    - 技术
    - AI大模型
tags: [大模型演进, AI模型, NLP, transformer, GPT]
---

验证在NLP领域，**预训练+微调**的方式可以得到很好结果。  
预训练：无监督，大数据集  
微调：有监督，小数据集

# 模型结构

<img src="/image/2025/image-gpt1.png" width=900 height=500 />
由图可见，模型使用了Transformer中Decoder的Masked-self-Attention结构/相当于剔除了CrossAttention模块的Decoder。  

模型由$N=12$层Decoder组成，最上层拼接Softmax层
<!-- more -->

# 数据
使用了7000本未出版的书BooksCorpus。大约1B/10亿词。其规模和ELMo类似
在数据应用上，ELMo对句子顺序作了打乱，破坏了模型的长距离信息提取能力，
GPT则使用更长的窗口，规避了这种问题。

# 模型训练
## 预训练
$$L_1(U)= \sum_i logP(u_i|u_{i-k},...,u_{i-1};\Theta)$$
使用以上损失函数，实际就是“下一词预测任务”。其中$U$为预训练数据集  
{% raw %}
$$\begin{alignedat}{2}
h_0 &= UW_e+W_p\\
h_l &=transformer\_block(h_{l-1})\\
P(u) &=Softmax(h_n W_e^T) 
\end{alignedat}$$
{% endraw %} 
其中：  
$n=$ 层数12  

$W_e=$ 词嵌入矩阵，注意最后Softmax层也是使用的该权重  

$W_p=$ 词位置嵌入矩阵  

## 微调
针对不同微调任务，设计不同的最上层Softmax权重$W_y$  
预测
$$P(y|x^1,...,x^m) = Softmax(h_n W_y)$$
下面是微调中的任务损失函数，其中$C$为微调数据集：
$$L_2(C)= \sum_{(x,y)} logP(y|x^1,...,x^m)$$
而论文中又提到，如果在微调中，同时做“下一词预测任务”，可以提高模型泛化能力，故设计最终微调损失：
$$L_3(C)= L_2(C) + \lambda * L_1(C)$$

## 微调中不同任务的处理
如果要一个模型解决所有NLP问题，又因为微调时不同任务有不同的输入格式，  
所以作者需要设计一种方法能**把各种NLP任务的输入转换成和预训练时相同的输入格式**
针对文本分类的数据，其实和预训练数据是一样的，很契合“预测下一词”的模式。但像文本蕴含，文本相似判断等任务，就需要转变：
这种转变，也在上图中有展示
1. 文本蕴含任务
作者设计了一个分隔符，提示模型分隔符前后代表不同含义
2. 文本相似任务
相似任务为了体现对称性，分别独立计算[t1,t2]和[t2,t1]的模型表示，然后把两个向量相加，再计算输出分数
3. 问答任务
把contenx+question+分隔符+不同answer拼接，分别独立计算模型表示，计算linear输出，再通过softmax


# 模型参数
<style>
  table {
    width: 60%;
    font-size: 12px;
  }
  th, td {
    padding: 2px;
    text-align: center;
  }
</style>
|Decoder层|注意力头|维度|前馈维度|参数量|词表|
|--|--|--|--|--|--|
|12|12|768|768*4|0.1B=1亿|4w-BPE|

**预训练时：**
|优化器|激活函数|lr|seqlen|batch|epoch|dropout|L2正则
|--|--|--|--|--|--|--|--|
|Adam|GELU|max=2.5e-4|512|64|100|0.1|0.01|

其中学习率是线性增到max（2000step）再cosine衰减到0

**微调时：**
|lr|batch|epoch|$\lambda$|
|--|--|--|--|
|6.25e-5|32|3|5|


# 讨论
## 论文出发点
为了减轻NLP领域对监督数据训练模型的依赖，在原始数据（raw data）中高效学习是至关重要的。  
但当时大多数NLP任务都是使用标记数据作监督训练，这样会导致：
- 标注数据总是不够多
- 在缺乏标注数据的领域，效果差

作者对于大量未标注数据做预训练有信心，是因为看到了**当时大家都会通过使用/训练更好的词向量来提高任务表现（例如ELMo）**。而词向量的训练就是大规模无监督学习得来。  
而预训练+微调的模式在CV领域早已验证有效。
1. 由此论文的**核心思想**就已确定：**设计一个预训练与微调统一的NLP模型**
2. 而作者也敏锐发现，**Transformer**在NLP领域相对于RNN的优势。于是GPT的雏形就有了。  
3. 作者选择了**Masked-self-Attention**，因为他使用了类似N-gram的语言模型：$L(u)= \sum_i logP(u_i|u_{i-k},...,u_{i-1};\Theta)$，这种“预测下一词”的模型，使其更侧重于文本生成任务。   

*或许作者最初就看到“文本生成”才是NLP的终极解决方案？*
