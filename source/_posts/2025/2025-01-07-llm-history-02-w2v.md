---
title: 大语言模型演进史-02-Word2Vec
mathjax: true
date: 2025-01-07 21:20:23
categories:
    - 技术
    - AI大模型
tags:
    - 大模型演进
    - AI模型
    - NLP
    - word2vec
    - doc2vec
---
Word2Vec是对NNLM的优化，提高了训练速度。
<img src="/image/2025/image_w2v_1.png" width=500 height=300 /> 
上图是神经网络语言模型的架构，是一个简单的三层神经网络  
训练语言模型的核心是构建词$w$与其周围词$Context(w)$的关系
1. NNLM使用的是N-gram模型即$Context(w_i)=(w_{i-N+1},w_{i-N+2},...,w_{i-1})$
所以NNLM的输入就是被预测词$w_i$的前N个词的向量拼接。  
2. 拼接的向量$X_w$经过一层网络$W$得到$Z_w=tanh(W \cdot X_w + p)$
3. $Z_w$再经过一层网络$U$得到$Y=softmax(U \cdot Z_w + q)$
<!-- more -->

# word2vec相比NNLM的改进
## 语言模型的改进
此处的“语言模型”专指如何构建词$w$与其周围词$Context(w)$的关系，更像是一个训练框架。例如N-gram就是前N个词预测第N+1个。其实还有其他方法去构建，例如不仅用前几个词，还要用后几个词。 

在Word2Vec中，有一个**窗口Window**的概念，即对于当前词$w_i$向前看n个词，向后看n个词，这个n即窗口大小。  
因为在自然语言中，一个词的意义是和其前后都有关系的。所以前后词都看，有助于理解当前词的实际意义，也就可以训练更好的词向量。  
而针对这$2n$个词，也有两种不同的“语言模型”即**连续词袋模型CBOW和跳词模型Skip-gram**。

### 连续词袋模型 CBOW(Continuous Bag-of-Words)
Bag-of-Words即把词装在一个袋子里。数学语言即把一组词的向量相加/平均。
Continuous的意思即：使用的是预测词前后窗口内的词  
<img src="/image/2025/image_w2v_2.png" width=500 height=300 />  
CBOW就是把这2n个词的向量sum起来，进行下一步运算
这个NNLM的N-gram很相近，词数由n变成2n，组合方式由拼接变为相加--这一步是可以减少计算量的。

### 跳词模型 Skip-gram
和CBOW反过来，用当前词去预测窗口内的2n个词
由这一步也可以看到，
训练一次Skip-gram，需要单独预测2n次
而训一次CBOW，只需预测1次。所以CBOW训练就会快，但Skip-gram就会训练的比较好。

## 输出阶段的改进
因为使用原始的softmax方法，NNLM的输出层需要预测词表中所有词的概率，这一步十分消耗计算。
为了减少这一步的计算量，需要想一个折中的办法。
Word2Vec使用的即层次softmax(Hierarchical Softmax)和负样本采样(Negetive sampling)

### 层次softmax Hierarchical Softmax
softmax的过程就是在n个结果中查找1个的过程。当n很大时，为了不逐一去寻找，就需要对n做分层/分块。  
层次softmax就是把n个结果构架在一个二叉树(Huffman树)中，每个词都在这棵二叉树的叶子节点，相当于把一个复杂度为$N$的问题变成了$log(N)$
<img src="/image/2025/image_w2v_3.png" width=500 height=300 /> 
通过提前构造好Huffman树，对于每一个被预测词，变成了**预测到达该词的路径**的问题。  
对于每一个非叶子节点（途中黄色节点）都有一个权重向量$w$
对于一个节点，我们约定向左子树走是1，向右子树走是0
<img src="/image/2025/image_w2v_4.png" width=500 height=300 /> 
例如词“观看”其路径是110
即对于该路径上经过的3个非叶子节点（a,b,c），需要依次得到1，1，0的输出
$p(path_1)= \sigma(x \cdot w_a)$--代表**向左**走的概率  
$p(path_2)= \sigma(x \cdot w_b)$--代表**向左**走的概率  
$p(path_3)= 1- \sigma(x \cdot w_c)$--代表**向右**走的概率  
最终预测“观看”的结果即连乘起来
$$p("观看"|x)= \prod_{i=1}^{n} p(path_i) $$

### 负样本采样 Negetive sampling
负样本采样的思想是：在N个词中选择1个被预测词A，相对于A，其他所有词都是负样本，当正负样本十分悬殊的时候，就需要对负样本做下采样，即每次预测，减少负样本数量。
具体如何减少：随机抽取几个负样本。这可以大幅提高性能，因为即减少了大幅计算，又不用建二叉树。

#### 随机采样方法
因为每个词的词频不同，需按照词频权重随机采样，高频词需要更容易被采到。
Word2Vec使用的方法是，**画一条线段，每个词占据其词频对应的长度，每次随机一个数，落在哪个区间，就选择哪个词**
为了加速计算，可以对这条线段细分成m份，构建一个字典，可以快速查询。

具体代码里，词频使用的是
$$f(w_j)=\frac {count(w_j)^{0.75}} { \sum count(w_i)^{0.75}}$$
目的是减小高频词的影响，防止高频词过多地被采样到。

### 其他

#### 对于数据集中高频词低频词的处理
**低频词** 设置最小词频，min_count,过滤低频词
**高频词** 对于高频词，同样需要下采样来降低频率  
$$prob(w) = 1- \sqrt {\frac{t}{f(w)}}$$
其中$prob(w)$代表词$w$被舍弃的概率，$f(w)$即词频，$t$是设置的参数例如$t=1-e5$
从这个公式可知，词频越大，$prob(w)$越大。当$f(w)<=t$时，该词不被舍弃。

