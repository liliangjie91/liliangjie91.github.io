---
title: 古早博文-深度学习-RNN-循环神经网络
date: 2020-12-31 22:26:13
categories: 
    - 技术
    - 机器学习
tags:
    - 机器学习
    - 神经网络
    - 深度学习
mathjax: true
---
### 1，结构
![图1-左侧为折叠形式，右侧为展开形式](/image/image-rnn1.png)
RNN接收序列数据为输入，何为序列数据？一句话就是一组词的序列，其中词有固定的顺序。一段语音是一组时间切片后语音信号的序列。例如“what is RNN”就可以看成序列输入
$x^{(0)}=what,x^{(1)}=is,x^{(2)}=RNN$
知道这点之后，就可以把RNN想象成一个机器，每个时刻，吃进一个词，吐出一个结果。这仿佛没什么特别。其实如果把上图左侧W删掉，这就是一个简单的DNN。但是，RNN为了能够计算序列中也就是词与词之间的联系，对隐藏层做了一个小修改，那就是隐藏层要循环改变自己。
这就是循环的由来。这个状态层的值我们称之为隐藏状态$h^{(t)}$，$h^{(t)}$相当于序列前面部分的记忆，记下了前面词的信息，然后再结合当前词来更新自己成为$h^{(t+1)}$。
具体地，一个rnn有3部分：
* 输入层$x$，它通过一组连接权重$U$接入状态层。变成 $Ux^{(t)}$
* 隐藏层$h$，它就厉害了。他有两个输入，一是输入层$Ux^{(t)}$，二是上一个自己$h^{(t-1)}$。再通过激活函数变成$h^{(t)} = \sigma(z^{(t)}) = \sigma(Ux^{(t)} + Wh^{(t-1)} +b)$
所以，RNN的隐藏层，是在反复更新自己的。
* 输出层$o$，这个就是简单的对隐藏层的加权
$o^{(t)} = Vh^{(t)} +c$
$y^{(t)} =\sigma(o^{(t)})=\sigma(Vh^{(t)} +c)$

**上述$U,V,W$是同一组$U,V,W$，不同时刻，RNN使用的是同一组$U,V,W$,即共享**

通过上面可以看出，所谓循环神经网络，循环的是隐藏层$h$。它是RNN最重要的一个部分。
*另外，要有“时刻$t$”这个概念。图1右侧是RNN的展开模式，看起来仿佛又很多网络连接，非常复杂，但实际上，这只是不同时刻的RNN展现在一起了而已。实际上他就是左侧的样子。就像一个人一天吃3顿饭，但是是在不同时刻吃的，不能想象成3个自己同时在吃早中晚饭。*

通俗来讲，假设RNN星球的小R是一个RNN代表。他要吃一堆水果，那么过程就是：
* 吃下第一个水果$x^{(0)}$
* 拉出第一个shit $h^{(0)}$
* 观察拉出的shit得出水果的名字 $y^{(0)}$
* 吃下第二个水果$x^{(1)}$
* 然后吃下第一个shit $h^{(0)}$
* 拉出第二个shit $h^{(1)}$
* 观察拉出的shit得出水果的名字 $y^{(1)}$
...

这个小R，可以适合不同任务，
例如输入一个序列，输出同等长度的序列，就像上面吃水果一样
也可输入一个序列，输出单一值(是否吃饱)，那么，按上述例子，他可以只在最后一个时刻计算输出层，而在这之前不必计算输出层。

### 2，前向传递与反向传播
#### 2.1 前向传递
如上所述
$h^{(t)} = \sigma(z^{(t)}) = \sigma(Ux^{(t)} + Wh^{(t-1)} +b)$
$y^{(t)} =softmax(o^{(t)})=softmax(Vh^{(t)} +c)$

#### 2.2 反向传播
![图2-左侧为折叠形式，右侧为展开形式](/image/image-rnn1.png)
依旧还是拿这个图来说。
上图中
$y$为真实值，$y'=softmax(o)$
$y$和$y'$经过计算得到损失$L$
此处我们定义损失函数为交叉熵损失$L^{(t)}=-\sum_{i=1}^{n}y_i^tlog(y^{'t}_{i})$。因为$y$是一个列向量(例如onehot编码) 
总损失$L=\sum_{t=1}^{T}L^{(t)}$

##### 2.2.1 求关于V，c的梯度$\frac{\partial L}{\partial V}$,$\frac{\partial L}{\partial c}$
跟V,c有关的：
$y^{(t)} =softmax(o^{(t)})=softmax(Vh^{(t)} +c)$
则
{% raw %}
$$\begin{alignedat}{2}
\frac{\partial L^{(t)}}{\partial V}&=\frac{\partial L^{(t)} }{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial V}\\
&=\frac{\partial L^{(t)} }{\partial y^{'(t)}}\frac{\partial y^{'(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial V}\\
&=[0,0..., -\frac{1}{y'_i},...,0][matrix](h^{(t)})^T \\
&=(y'^{(t)}-y^{(t)})(h^{(t)})^T \\\\
\frac{\partial L^{(t)}}{\partial c}&=(y'^{(t)}-y^{(t)})
\end{alignedat}$$
{% endraw %}
上面第三个等号出涉及到softmax求导，具体在下面详述。

##### 2.2.2 求关于W,U,b的梯度$\frac{\partial L}{\partial W}$,$\frac{\partial L}{\partial U}$,$\frac{\partial L}{\partial b}$
跟W,U,b有关的：
$h^{(t)} = \sigma(z^{(t)}) = \sigma(Ux^{(t)} + Wh^{(t-1)} +b)$
此处约定激活函数是$tanh$函数，导数为$(1-tanh^2)$
所以，我们可以求$$\delta^{(t)}=\frac{\partial L}{\partial h^{(t)}}$$
而对于$h^{(t)}$，看上图，它有两个走向，一路向$o^{(t)}$，一路向$h^{(t+1)}$
所以
{% raw %}
$$\begin{alignedat}{2}
\delta^{(t)}&=\frac{\partial L}{\partial o^{(t)}}(\frac{\partial o^{(t)}}{\partial h^{(t)}} )^T + \frac{\partial L}{\partial h^{(t+1)}}( \frac{\partial h^{(t+1)}}{\partial h^{(t)}} )^T \\
&= V^T(\hat{y}^{(t)} - y^{(t)}) + W^T\delta^{(t+1)}diag(1-(h^{(t+1)})^2)\\\\
\delta^{(T)}&=( \frac{\partial o^{(T)}}{\partial h^{(T)}})^T\frac{\partial L}{\partial o^{(T)}} \\
&= V^T({y'}^{(T)} - y^{(T)})
\end{alignedat}$$
{% endraw %}
则
{% raw %}
$$\begin{alignedat}{2}
\frac{\partial L}{\partial W}&= \sum\limits_{t=1}^{T}diag(1-(h^{(t)})^2)\delta^{(t)}(h^{(t-1)})^T\\
\frac{\partial L}{\partial U}&=\sum\limits_{t=1}^{T}diag(1-(h^{(t)})^2)\delta^{(t)}(x^{(t)})^T\\
\frac{\partial L}{\partial b}&= \sum\limits_{t=1}^{T}diag(1-(h^{(t)})^2)\delta^{(t)}\\
\end{alignedat}$$
{% endraw %}
~~注意此处的$diag(1-(h^{(t)})^2)$ 是tanh函数的导数，他应该是一个列向量：
$[1-(h^{(t)}_0)^2,1-(h^{(t)}_1)^2,1-(h^{(t)}_2)^2,...,1-(h^{(t)}_n)^2]^T$
而他所参与的乘法运算，往往是$\odot$而不是$\cdot$ 。这和sigmoid函数一样。
而softmax函数则和tanh，sigmoid不同。softmax函数需要综合所有输入才能获得最终输出，而后两者一个输入就能获得对应的输出。~~

##### 2.2.3 softmax 函数及导数
* 函数
$x=[x_0,x_1,x_2,...,x_n]$
$y=softmax(x)=[\frac{e^{x_0}}{\sum_{k=0}^{n}e^{x_k}},\frac{e^{x_1}}{\sum_{k=0}^{n}e^{x_k}},...,\frac{e^{x_i}}{\sum_{k=0}^{n}e^{x_k}},...,\frac{e^{x_n}}{\sum_{k=0}^{n}e^{x_k}}]$

* 导数
y是一个向量，x也是一个向量，向量对向量求导，得到一个矩阵$[\frac{\partial y_i}{\partial x_j}]_{n\times n}$
  * $i=j$时
$\frac{\partial y_i}{\partial x_j}=\frac{-e^{x_i}e^{x_j}+e^{x_i}\sum_{k=0}^{n}e^{x_k}}{(\sum_{k=0}^{n}e^{x_k})^2}=\frac{e^{x_i}}{\sum_{k=0}^{n}e^{x_k}}\cdot \frac{\sum_{k=0}^{n}e^{x_k}-e^{x_i}} {\sum_{k=0}^{n}e^{x_k}}=y_i(1-y_i)$

  * $i\neq j$时
$\frac{\partial y_i}{\partial x_j}=\frac{-e^{x_i}e^{x_j}+0\sum_{k=0}^{n}e^{x_k}}{(\sum_{k=0}^{n}e^{x_k})^2}=\frac{e^{x_i}}{\sum_{k=0}^{n}e^{x_k}}\cdot \frac{-e^{x_j}} {\sum_{k=0}^{n}e^{x_k}}=-y_iy_j$

  总结来说，对于导数矩阵来说，对角线元素是$y_i(1-y_i)$，非对角线元素是$-y_iy_j$

未完待续
### 3，参考
[https://www.cnblogs.com/pinard/p/6509630.html#!comments](https://www.cnblogs.com/pinard/p/6509630.html#!comments)

