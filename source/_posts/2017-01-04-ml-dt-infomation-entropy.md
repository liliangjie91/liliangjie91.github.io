---
title: 古早博文-决策树-信息论基础知识-信息量-熵-条件熵-信息增益
date: 2017-12-31 20:08:22
categories: 
    - 技术
    - 机器学习
tags:
    - 机器学习
    - 集成学习
mathjax: true
---
### 1，信息论重要概念简述
#### 1.1自信息（self-information）I(xi)
> 当信息被拥有它的实体传递给接收它的实体时，仅当接收实体不知道信息的先验知识时信息才得到传递。如果接收实体事先知道了消息的内容，这条消息所传递的信息量就是0。只有当接收实体对消息的先验知识掌握少于100%时，消息才真正传递信息。

如何理解这句话？
自信息应当从消息接受者的角度来看，
直观解释即
如果A告诉B一个B已经知道的事情，那么在B看来，A说的就是废话，信息量=0
如果A告诉B一个B不知道的事情，但B大概率能猜出来（如国足没进世界杯），那么在B看来，A虽然说了一个他不知道的消息，但这个消息携带的信息比较少，因为在他的观念里，国足没进世界杯已经发生了80%了，A的话提供了20%的信息。
但如果A告诉B国足进世界杯了，那么A的话就提供了80%的信息。对于一个对国足失望透顶的C来说就提供了90%的信息。对于一个压根儿不相信国足的D来说，就提供了100%的信息。

所以，一件事a发生所携带的自信息，和a发生的概率有直接关系。即
如果$P_{a}=0.9$很大，则当a发生了，这件事也没提供很大信息
如果$P_{a}=0.1$很小，则当a发生了，这件事就提供了很大的信息。
$$I(x)=-logP_{x}=log(\frac{1}{P_{x}}) $$
这样看来，这个公式比较合理了，但有人会问，为什么不直接让$I(x)=\frac{1}{P_{x}}$呢？
这里就要考虑自信息的其他性质了
例如，两件相互独立的事a，b同时发生，那么这件事的自信息应该是什么呢？设计者觉得应该是相加即
$$I(a,b)=I(a)+I(b)$$
这听着很有道理，毕竟国足进世界杯和明天下雨这两个消息的信息量确实感觉应该是相加的
而
{% raw %}
$$
\begin{alignedat}{2}
I(a,b)&=log(\frac{1}{P_{(a,b)}}) \\
&=log(\frac{1}{P_{a}P_{b}}) \\
&=log\frac{1}{P_{a}}+log\frac{1}{P_{b}}\\
&=I(a)+I(b)
\end{alignedat}
$$
{% endraw %}
如果直接让$I(x)=\frac{1}{P_{x}}$，则不满足上述等式。
所以，自信息就被设计成了
$$I(x)=-logP_{x}=log(\frac{1}{P_{x}})$$

#### 1.2熵
定义：熵是随机变量自信息的期望
$$H(X) = \sum\limits_{i=1}^{n}p_i I(i)=-\sum\limits_{i=1}^{n}p_i logp_i$$
熵度量的是事件的**不确定性**熵越大，不确定性越大
熵又叫乱度，很直观，熵越大，越乱。越乱，熵越大。
那么熵和随机变量的概率分布又有什么关系呢，仅凭公式不太直观？
我们以袜子在哪里X为例（小明会把袜子放在a旧衣篓，b床，c沙发3个位置）
今天小明很乖，把所有袜子都放进了旧衣篓a里。
即$P_{a}=1,P_{b/c}=0$
则
$$\begin{alignedat}{2}
H(X)&=-\sum\limits_{i=a,b,c}p_i logp_i\\
&=-(1*0+0+0)\\
&=0
\end{alignedat}$$

小明他妈进屋一看，很开心，说：“小明你做的很好，袜子位置的不确定性=0 乱度=0，归置的不错”。
小明听后说：“是的妈妈，**现在我能准确说出所有袜子的位置**”
之后小明变懒，一个月后，他的15双袜子被扔到到处都是，a,b,c 各自5双
$$\begin{alignedat}{2}
H(X)&=-\sum\limits_{i=a,b,c}p_i logp_i\\
&=-(\frac{1}{3}log\frac{1}{3}+...+\frac{1}{3}log\frac{1}{3})\\
&=log(3)\\
&\approx 1.58
\end{alignedat}$$
小明他妈进屋一看，很生气，说：“小明你做的很差，袜子位置不确定性达到最大，乱死了”。
小明听后说：“是的妈妈，**现在我难以说出某一双袜子的具体位置，因为它可能在这3个位置的任意一个，更糟糕的是，它出现在这3个位置的概率是相等的，更加难猜了。早知道我就把沙发上的袜子扔到床上了，这样我还可以无脑猜在床上，会好猜一点**”
此时小明爸爸进屋说：“很好，你发现了吗，**当袜子的分布越平均时，熵就会越大，分布越不平均，熵就会越低，极端情况，所有袜子集中在一点时，熵就会变为0。**”
小明妈妈说：“别说了，去拿鸡毛掸子吧”

注意：**熵和随机变量的分布相关，同时也和随机变量的数量n有关系**
例如投硬币正反随机，投骰子1-6随机，但是投硬币的熵log(2)<投骰子的熵log(6)
#### 1.3条件熵
定义
$$\begin{alignedat}{2}
H(X|Y) &= -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i|y_i) \\
&= \sum\limits_{j=1}^{n}p(y_j)H(X|y_j)
\end{alignedat}$$
<!-- ![image.png](/image/image-entropy1.png) -->
<img src="/image/image-entropy1.png" width=300 height=150 />
即，知道Y分布的前提下，X的熵；也可理解为，在知道Y后X剩下的熵
还是以袜子的位置为例：
小明见他妈拿着鸡毛掸子向他走来
连忙说：“且慢，我的袜子是有颜色的，虽然现在散落在3个位置，但你仔细看，5双红色的都在a里，5双黑色的都在b上，5双白色的都在c上，令Y代表袜子颜色，则有”
$$\begin{alignedat}{2}
H(X|Y)&= \sum\limits_{j=red,black,white}p(y_j)H(X|y_j)\\
&=\frac{1}{3}*0+\frac{1}{3}*0+\frac{1}{3}*0 \\
&=0
\end{alignedat}$$
<!-- ![image.png](/image/image-entropy2.png) -->
<img src="/image/image-entropy2.png" width=300 height=150 />
"所以您看，**在知道袜子颜色后，我依旧能准确说出袜子的位置**。很确定"
妈妈见状说：“哇哦，虽然一开始袜子位置的熵很大，但在知道袜子颜色的分布后，袜子位置的条件熵变成了0”
爸爸附和道：“是啊，**条件熵和条件概率很像，都是在知道另一个分布后，影响了当前分布的值**，熵和概率还真是有很多相通之处呢。快把鸡毛掸子收起来吧”


#### 1.4信息增益／互信息（Mutual-Information）I(X;Y)
定义：
$$I(X;Y)=H(X)-H(X|Y)$$

小明见妈妈收起了鸡毛掸子，便又哔哔起来：“你看我这次按颜色分了袜子，袜子位置和袜子颜色的互信息=H(X)-H(X|Y)=H(X)=1.58，也就是说袜子颜色这个特征带来的信息增益是袜子位置的熵的100%。
但你看我的袜子又分为
3双足球袜l，6双短筒袜m和6双船袜s，
3双足球袜l都在旧衣篓a里，
6双短筒袜m平均分在了a,b,c中，
6双船袜s有4双在床上，2双在沙发上。
所以用Z表示袜子类型 则”
{% raw %}
$$\begin{alignedat}{2}
H(X|Z)&= \sum\limits_{j=l,m,s}p(z_j)H(X|z_j)\\
&=\frac{3}{15}0+\frac{6}{15}(\frac{2}{6}log(3)+\frac{2}{6}log(3)+\frac{2}{6}log(3))+\frac{6}{15}(\frac{4}{6}log(\frac{6}{4})+\frac{2}{6}log(3)) \\
&=0+\frac{6}{15}log(3)+\frac{2}{15}log(3)+\frac{4}{15}log(\frac{6}{4}) \\
&=\frac{12}{15}log(3)-\frac{4}{15}\\
&\approx 1\\\\
I(X;Z) &=  H(X)-H(X|Z)\\
&\approx0.58
\end{alignedat}$$
{% endraw %}
“所以，袜子类型这一特征可以为袜子位置分布带来约0.58的信息增益，要小于颜色的作用。所以如果要预测袜子的位置，我会优先获取袜子的颜色信息，这样更可能快速获得准确结果。”
“什么？你怎么把短筒袜和船袜乱放！”气愤的妈妈拿着鸡毛掸子走向了小明。。。




