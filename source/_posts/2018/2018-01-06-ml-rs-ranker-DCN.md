---
title: 古早博文-推荐系统-重排序-CTR-DCN-CIN-xDeepFM
date: 2018-12-31 21:31:33
categories: 
    - 技术
    - 推荐系统
tags:
    - 机器学习
    - 推荐系统
    - 排序模型
mathjax: true
---
一个总结
[https://www.cnblogs.com/xianbin7/p/10661572.html](https://www.cnblogs.com/xianbin7/p/10661572.html)

#### 特征交叉的 元素级bit-wise VS 向量级vector-wise
元素级别的特征交叉，典型的就是神经网络。向量级特征交叉典型的就是向量点乘。
$$a=(a_0,a_1,a_2),b=(b_0,b_1,b_2)$$

$$Cross_{bit}(a,b)=\sum w_ia_ib_i$$

$$Cross_{vec}(a,b)=w \sum a_ib_i$$
两者区别就是，对于特征交叉来说，从直观上，vector-wise的形式更能理解。而bit-wise则显得更加隐晦。同时，vector-wise会引入更少的参数。

我们的目的是引入更多的交叉特征，至于交叉特征的权重，我们交由模型去学习。但是，有效地提供交叉特征这一步，有很多困难。
FM实现了二阶交叉特征。但是，当涉及更高阶的交叉特征时，就会因为参数太多而乏力。
DNN实现了更高阶的特征交叉，但是，这些特种特征交叉是bitwise的，同样也有参数过多的问题。
现在我们思考这个问题。要实现特征交叉，就必然会面临指数级增长的参数。但是，并不是所有特征交叉都是有用的。如果我们能使用高阶交叉特征，同时又减小参数，那么就需要做一件事---压缩。把高阶特征数量压缩下来。

#### DCN(Deep&Cross Network )
![dcn.png](/image/image-dcn.png)
这里最关键的就是中间左侧黄点框。即cross-network
$$x_{l+1} = x_0 x_l^T w_l + b_l + x_l$$
$$x_1=x_0x_0^Tw_0+b_0+x_0$$
这里面$x_l,w_l,b_l$ 都是列向量即$shape=(n,1)$  

$x_0=(x_{01},x_{02},...,x_{0n})^T$

![alt text](/image/image-gongshi.png)

这些推导下来，在中间发现确实有特征交叉，但是最后发现，因为$a^l$是实数，所以最终变成了$x_0$ 的倍数变化。即高阶特征交叉和一阶特征有很大的相关。

这说明DCN虽然可以自如地控制和使用高阶特征交叉，但是在高阶特征交叉方面还是有一定局限性的。同时特征交叉依旧是bitwise的。

#### CIN 压缩交互网络
一个m*D的矩阵 $X^0$ 
m是初始稀疏特征数
D是每一个特征的Embedding维度
$X^0$就是m个Embedding向量组合
表示未做特征组合的原始向量

$X^{k-1}$表示做k-1阶特征组合，它有$H_{k-1}$个D维向量组成
那么
$X^{k}$是由$X^{k-1}$ 与 $X^{0}$ 以某种形式组合而成的，具体来说  

$X^{k-1}$ 与 $X^{0}$ 相当于分别$H_{k-1}$个和$m$个D维向量  

那么$X^{k-1}_1$ 与$X^{0}_1$分别代表第一个k-1阶特征的D维向量和第一个原始向量  

定义$$Z^{k}_{11}=X^{k-1}_1 \circ X^0_1 = (X^{k-1}_{11}X^0_{11},X^{k-1}_{12}X^0_{12},...,X^{k-1}_{1D}X^0_{1D}) $$
$Z^k_{11}$也是一个D维向量
这样的话，因为$X^{k-1}$ 与 $X^{0}$ 分别有$H_{k-1}$个和$m$个D维向量
那么就有$H_{k-1} * m$个$Z^k_{ij}$
则$Z^k$就是一个$D * m * H_{k-1}$的三维矩阵(张量)
而现在的目的是，把这个三维张量压缩成$H_{k}*D$维的二维向量
![CIN.png](/image/image-CIN1.png)
$Z^k$也可以看成$m * H_{k-1}$个D维向量。我们对这$m * H_{k-1}$个D维向量用$W_{ij}$加权求和会得到一个D维向量。
这样，我们设置$H_k$个$W$分别做加权求和，就能得到$H_k$个D维向量。这样，就完成了压缩。得到了而$X^k$

![CIN2.jpg](/image/image-CIN2.png)
最终得到1阶2阶。。k阶的特征组合，每一阶都有$H_k$个D维向量。
最后把D维sum成1维，我们就得到了$$\sum^{k}_{i=1}H_i$$ 个数，这些数组成一个向量p
即上图的黄点方框

CIN 结合了CNN的思想。不是把特征的Embedding向量拼接组成一行，而是组成了一个矩阵。特征交叉变成了两个矩阵的操作。这个操作把结果变成了一个3维矩阵。然后在把3维变2维的过程中，使用了卷积层的概念。使用$H_l$个卷积核每个卷积核可以把一个3维矩阵变成1维向量(长度为D)。在最后一步，使用了pooling层的概念。
CIN实现了vector-wise的高阶特征交叉，同时带有压缩。

#### xDeepFM
理解了CIN就比较好理解xDeepFM 了
![xdeepfm.png](/image/image-xdeepfm.png)
### 参考
[http://xudongyang.coding.me/xdeepfm/](http://xudongyang.coding.me/xdeepfm/)
