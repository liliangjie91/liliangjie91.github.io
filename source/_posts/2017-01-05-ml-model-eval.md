---
title: 古早博文-机器学习-模型评价指标
date: 2017-12-31 20:09:50
categories: 
    - 技术
    - 机器学习
tags:
    - 机器学习
mathjax: true
---
对于机器学习模型性能的评价，我们最常见的指标就是准确率(Accuracy,ACC)，当然，这是针对分类的。那么，除此之外，还有很多其他的指标。
前面我们列出的损失函数，其实在某种意义上也是对模型的一种评价指标。

### 1，二分类模型的评价指标
#### 基础知识
对于二分类而言，样本有正负样本之分(Positive，Negative)，而预测则有对错之分(True,False)，所以可用过如下表来表示
||真实值：P|真实值：N|
|---|---|---|
|预测值：P|TP|FP|
|预测值：N|FN|TN|
则
>TP：真正例，真阳性。样本是正例，预测为正，分类正确
FP：假正例，假阳性。样本是负例，预测为正，分类错误。误诊
TN：真负例，真阴性。样本是负例，预测为负，分类正确
FN：假负例，假阴性。样本为正例，预测为负，分类错误。漏诊

#### 准确率(Accuracy,ACC)
$$ACC=\frac{TP+TN}{TP+TN+FP+FN}=\frac{T}{T+F}$$
即分对样本数比总体样本数
计算简单，常见
**缺点**
1，当样本不平衡时会不准。例如正负样本9:1，模型把所有样本均判为正，计算出ACC=0.9 但这个模型是没有意义的。
此时，可以计算平均准确率，即每个类别内准确率的平均值。
2，太过简单，不能体现正负样本具体的分类对错情况

#### 精确率(Precision,P)与召回率(Recall,R)
$$Precision=\frac{TP}{TP+FP}$$
即模型预测为正的样本里，真正为正的比例
$$Recall=\frac{TP}{TP+FN}$$
召回率也叫做敏感度(Sensitivity)
即在所有正样本中，模型准确找出的比例

P高代表模型预测为正，基本上就是正。表示其很准。但很准的原因可能是模型太严格，例如100个正例，模型只判断了其中1个为正，确实这个样本分对了，但是依旧错分了其他99个，造成假阴性变高。
R高代表模型更能够把正样本从样本中找出来，**漏诊率低，很敏感，稍微不对就会判正**。但例如模型把所有样本都判为正，此时召回率确实高，但没有意义。会带来很高的假阳性。

即高P很容易降低R，高R很容易降低P。两者需要权衡

#### F1 指数
$$F1=\frac{2*P*R}{P+R}=\frac{TP}{TP+(FP+FN)/2}$$
因为P和R很容易此高彼低，所以F1的分子是P*R，这就使得，盲目提高P和R其中之一，并不会提高F1指数，只有当两者都高的时候，F1才会高。第二个等号后面的式子也表明，F1指标旨在降低FP和FN(即假阳性和假阴性)两者。
##### [调和平均数](https://www.cnblogs.com/xiaobajiu/p/7867162.html)
F1实际是P和R的调和平均数，其特点是F1受PR中值小的那个影响更大，单一的某一个值大是没用的。
#### ROC 曲线(Receiver Operating Characteristic)
接收者操作特征曲线,是一种显示分类模型在所有分类阈值下效果的图表
此处先介绍两个指标
1，真正例率(TPR)--即召回率
$$TPR=\frac{TP}{TP+FN}$$
2，假正例率(FPR)
$$FPR=\frac{FP}{FP+TN}$$
与召回率相对应的，有一个叫做特异性(Specificity)的指标
$specificity=\frac{TN}{FP+TN}$ 表示在所有负样本中，模型准确找出的负样本比例。
由此可见，$FPR=1-Specificity$
我们当然希望TPR=1，FPR=0。
此时，正负样本均被正确分类
而ROC曲线的横纵坐标，分别是FPR(1-Specificity)，TPR
![ROC曲线](https://upload-images.jianshu.io/upload_images/3376541-b718e108b88694fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在做ROC曲线时，
首先，利用当前模型我们可以得到测试样本x对应的预测值f(x)，这f(x)可以是连续值而非输出的标签值(可以认为是尚未送入$sign()$函数的f(x))。
然后，把测试样本按照f(x)做升序排序
最后，对于排序后的样本及其f(x)，依次顺序从大到小卡阈值把样本分为两组，小于阈值的一组判为负样本，大于阈值的一组为正样本。
一开始，把阈值设为最大，此时所有样本为负样本，TPR=FPR=0
然后逐渐减小阈值，TPR和FPR都升高，对于一个较好的模型，此时应该TPR>FPR>0
然后逐渐把阈值减到最小，此时所有样本为正样本，TPR=FPR=1
ROC曲线(0,1)点代表着完美的分类与阈值。所以曲线越接近这一点越好。

#### AUC (Area Under ROC Curve ROC曲线下面积)

>AUC值为ROC曲线所覆盖的区域面积,显然,AUC越大,分类器分类效果越好。
AUC = 1，是完美分类器
0.5 < AUC < 1，优于随机猜测。分类模型妥善设定阈值可以取得好的效果 
AUC = 0.5，跟随机猜测一样
AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

#### 小结一下
![这张图可以仔细回味一下](https://upload-images.jianshu.io/upload_images/3376541-741990d4501f2106.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
由上图可见，
TPR=敏感度(Sensitivity)=召回率(Recall)=分母为正样本
FPR=1-特异性(Specificity)=分母是负样本
精确率(Precision)=分母是预测为正的样本

#### AUC问题深入探讨
大家都知道AUC是ROC曲线下面积，也知道ROC是怎么画出来的，也知道AUC可以通过公式计算
$$AUC=\frac{\sum (p_{i},n_{i})_{p_{i}>n_{i}}}{P*N}$$
但是，互相之间的推导则语焉不详。

从实际出发，ROC上的点是(FPR,TPR),
而TPR=TP/(TP+FN)=TP/P,FPR=FP/(FP+TN)=FP/N，两者分母是定值
而在实际绘图过程中，score倒序，每过一个样本，绘制一点。
因为一个样本(通常情况下)非1即0，故没过一个样本，TP和FP只会有一个值改变：
如果是正样本1，则TP+1，FP不变
如果是负样本0，则FP+1，TP不变
而且，每次变化只能是+1
如此反映到作图上就是：
如果是正样本，则向上走一个单位a=1/P；
如果是负样本，则向右走一个单位b=1/N。

那么，当求AUC时，即为求多个长方形面积之和
如此定义长方形：以当前点(A,B)为左上角，(1,B)为右上角，(A,B-b)为左下角，(1,B-b)为右下角
会发现，我们仅用正样本对应的点组成的长方形就刚好覆盖了面积
这个长方形的面积=(分数小于该正样本分数的负样本个数)*a*b

所以
AUC=[排序后，对于所有正样本，(分数小于该正样本分数的负样本个数)]*a*b
=[针对所有(正，负)样本对，Score(正)>Score(负)的个数]/(PN)

而据此，则可以发现AUC的物理意义即，正样本排在负样本之前的概率
他不在乎正样本之间如何排列，只在乎正样本是否排在负样本之前。这似乎更契合排序的意义

#### ROC与PRC
同时，ROC核心的两个量：在正样本中正确识别出正样本的概率a，1-在负样本中正确识别出负样本的概率b
分别照顾了正负样本。如此，对于一个模型，正负样本比例发生变化，并不会影响对应的概率ab，也就对AUC的值影响较小。这样，在模型连续训练过程中，auc的值不至于发生剧烈震荡。
而PRC，则不是，P=TP/(TP+FP)=预估为正样本的样本里确实为正样本的概率，R=TPR=TP/(TP+FN) 这两组值
正样本10个，负样本20个，正样本全部预估为正，负样本50%预估为正50%预估为负
则 R=1，P=10/(10+10)=0.5
正样本10个，负样本200个，正样本全部预估为正，负样本50%预估为正50%预估为负
则R=1，P=10/(10+100)=0.09
正样本10个，负样本2000个，正样本全部预估为正，负样本50%预估为正50%预估为负
则R=1，P=10/(10+1000)=0.0099
这导致正负样本比例差异大时，PRC很敏感，可能导致的情况是今天样本和昨天样本PRC差异很大。
但如果，我要训练一个风控模型，尽可能全的识别敏感词(高召回)，则可以看PRC，去看在R很高时例如R=0.95，P的值，然后调整模型保持在R不变时，提高P，例如把P从0.1提高到0.5，则
表示误审从0.9减少到0.5，既可以减少0.4的复审人力

#### AUC 与 GAUC
GAUC即每个用户的AUC加权平均，所谓的权可以使用户click数量等
为什么有这个指标？在推荐和广告模型中，排序阶段均使用AUC评估模型性能，AUC评价了模型整体排序能力即整体把正样本排在负样本前的能力。
但实际上，在真实的应用中，模型的每次预估，都是对同一个用户排一堆物品/广告。如此看GAUC似乎更贴切
但业内常用的还是AUC，为何？
1，惯性。GAUC是2017年阿里在DIN论文中提出的，此时AUC已经使用多年
2，AUC与GAUC目标并不相悖，提高AUC，大概率也会提高GAUC
3，用户点击序列是长尾分布的，即有大量低频用户，其AUC应该是比较容易震荡的，可能导致GAUC不稳定？
### 2，多分类模型的评价指标
#### 混淆矩阵(Confusion Matrix)
||真实类别1|真实类别2|真实类别3|
|---|---|---|---|
|预测1|500|20|100|
|预测2|10|480|200|
|预测3|10|50|370|
上表即混淆矩阵，详细描述了不同类别下样本预测情况。
由上表可以看出，样本分3个类别，每个类别样本分别520,550,670个。
那么此时，有一些指标可以评价多分类模型
#### ACC与平均ACC
ACC还是可以计算的，如上
$ACC=(500+480+370)/(520+550+670)=0.776$
平均ACC即各类别的准确率的平均
$ACC_{ave}=(500/520+480/550+370/670)/3=0.795$

#### 宏观与微观指标(Macro & Micro)
多分类混淆矩阵可以拆分成n个二分类混淆矩阵，如上可以拆分为如下3个
|1|真实类别1|其他|
|---|---|---|
|预测为1|500|120|
|预测为其他|20|1100|

|2|真实类别2|其他|
|---|---|---|
|预测为2|480|210|
|预测为其他|70|980|

|3|真实类别3|其他|
|---|---|---|
|预测为3|370|60|
|预测为其他|300|1010|
则可以按照二分类的方式获取3组指标，如ACC，Precision，Recall，F1等
甚至在获得预测样本每一类的概率值的情况下，做出3组ROC曲线
宏观指标Macro指算出上述3组指标之后，直接做平均

微观指标Micro指，把上述3表合成1张表，对应位置相加
|4|真实类别|其他|
|---|---|---|
|预测为真实类别|1350|390|
|预测为其他|390|3090|
通过这张表计算的指标如ACC，Precision，Recall，F1等，称为微观指标Micro


### 回归模型评价指标
回归模型评价指标通常会用到回归模型损失函数如MSE，MAE等
### 参考
[ROC曲线](https://blog.csdn.net/qq_37059483/article/details/78595695)
[模型评估与实践](https://www.jianshu.com/p/b4d40760156c)
[Macro与Micro](http://sofasofa.io/forum_main_post.php?postid=1001112)
